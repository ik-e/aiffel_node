{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock Scissor Paper toy project\n",
    "AIFFEL 대전 탐색 노드 1   \n",
    "가위바위보 게임에서 webcam으로 찍은 사람 손 모양(가위, 바위, 보) 사진을 분류하는 프로젝트이다.   \n",
    "프로세스는 다음과 같다.\n",
    "0. Import package\n",
    "1. Image preprocessing\n",
    "2. Load data\n",
    "3. Sampling\n",
    "4. Make model\n",
    "5. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task : make image classification for \"rock scissor paper\" on webcam\n",
    "#import package for project image classification\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "#import package for image processing\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image preprocessing (resize)\n",
    "이미지 사이즈를 28*28로 down sampling 한다.(아마도 계산량 절감 목적인 것 같다.(교육용이니까))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지  가위  디렉토리 경로:  ./scissor\n",
      "가위  이미지 resize 완료!\n",
      "이미지  바위  디렉토리 경로:  ./rock\n",
      "바위  이미지 resize 완료!\n",
      "이미지  보  디렉토리 경로:  ./paper\n",
      "보  이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "#path setting & name setting\n",
    "image_dir_path = [\"./scissor\", \"./rock\", \"./paper\"]\n",
    "data_name = [\"가위\",\"바위\",\"보\"]\n",
    "\n",
    "# resize 28*28 for all images\n",
    "for i in range(0,3):\n",
    "    print(\"이미지 \", data_name[i], \" 디렉토리 경로: \", image_dir_path[i])\n",
    "    images=glob.glob(image_dir_path[i] + \"/*.jpg\")  \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img,\"JPEG\")\n",
    "\n",
    "    print(data_name[i], \" 이미지 resize 완료!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "Train data와 test data는 다른 디렉토리로 분류하였고,   \n",
    "각각 5100, 1200개로 정확히 가위 바위 보 비율이 같다.(1:1:1)   \n",
    "한 사람당 300개(가위 100, 바위 100, 보 100)이므로 17명과 4명으로 분류한 데이터이다.   \n",
    "성능평가에 신뢰성을 높이기 위해서 train, test data에 같은 사람이 만든 데이터가 동시에 들어가지 않도록 하였다.   \n",
    "즉, test data는 train 과정에서 학습이 안된 다른 환경의 데이터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 5100 입니다.\n",
      "x_train shape: (5100, 28, 28, 3)\n",
      "y_train shape: (5100,)\n"
     ]
    }
   ],
   "source": [
    "#define data loading\n",
    "def load_data():\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=5100\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "    \n",
    "    idx=0\n",
    "    for file in glob.iglob('./scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob('./rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob('./paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "(x_train, y_train)=load_data()\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_test)의 이미지 개수는 1200 입니다.\n",
      "x_test shape: (1200, 28, 28, 3)\n",
      "y_test shape: (1200,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test를 만드는 방법은 x_train, y_train을 만드는 방법과 아주 유사합니다.\n",
    "# 경로 설정만 차이가 있음\n",
    "\n",
    "def load_test_data(img_path):\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    number_of_data=1200\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "    \n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'scissor0/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'rock0/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1       \n",
    "    \n",
    "    for file in glob.iglob(img_path+'paper0/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\",idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = \"./test/\"\n",
    "x_test,y_test = load_test_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling\n",
    "데이터의 확인하기 위한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the train data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[2])\n",
    "print('라벨: ', y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYwklEQVR4nO2de4ycZ3XGnzOzMzt79Xrj2xJv7k5CQCEEE9GmpaQpECLRECiXgCBVU8wfIIEUVdAUhAX/IMpFCEGpQ9IkFQRRkShBigppRIgBEWXjOLETx3FiO/Z67V3b2fttbqd/7IBM2Pd5l73MbPs+P2k1u/Ps+33vfPM9883Mec855u4QQvz/J9PoCQgh6oPMLkQiyOxCJILMLkQiyOxCJEJTPXdWKBS8o6M9qMfiApVKNajl8810bKlYpLpl+OteNhvWy+USHZvJZqnuHn5cczo/MmzuGeP7rvJdAzCqZjKR7Xv42Cw1EsRnBlSrlSXsm+vR8RG9WJwNatnI+ZIhz/fM9CyKxdK8h2ZJZjez6wB8C0AWwPfd/Svs/zs62nHDje8O6u786Rsdnw5qvb3n0rHH+geo3txSoHp7R0tQO3XqBB3b2d1B9WJxhuuRF5PmfGtQy+c66djZ2YhlPEfl1hb+2KYrQ0FtZoY/7sjrCJqMv0BPTY4HtVIpbDYAMPBXwfIsH1+t8Ofs2NFDQa2zkx/Tttbwufrb3+wNaot+G29mWQDfAfAuAJcBuMnMLlvs9oQQK8tSPrNfBeBFdz/o7kUAPwJww/JMSwix3CzF7GcDOHrG3/21+/4AM9tmZn1m1jcdedsmhFg5lmL2+T7s/dG3Eu6+w923uvvWlgL/XCyEWDmWYvZ+AL1n/L0ZAP8WTAjRMJZi9icAbDGz880sD+BDAB5cnmkJIZabRYfe3L1sZp8C8DPMhd7udPdn6Rjw+KRFQinF6fBn/tlIKGTt2rVUj8VNS6VwKKWtrY2OzUYiwhs3bqT61NQU1adnwnPL5/lT3FIIh+0AoDx/yPb3lIo8xNSUC++/uZmvjWiKrH2oVst8fFN435VIaKxS5tsuR/RCc57qTdmwHlu7kLHw42LP1pLi7O7+EICHlrINIUR90HJZIRJBZhciEWR2IRJBZhciEWR2IRJBZhciEeqaz54xQ3MhHF9syvJ0yo7OcDy7kIvENdu5Xozku7OYbkdnNx07eOoY3/cMj6Pnm/hrcqY1nH4by+PPNvFYd1tLuP4AAIwWJ6ne2hqO41tkbUM1kmwfS1PNWHj7hTw/10plvu1KZI0A2zcA5PPh83Ep+eywcKRdV3YhEkFmFyIRZHYhEkFmFyIRZHYhEkFmFyIR6hp6A3gq6eQkD+N0dobDQLkcD1dMT/LwFg1nAGhvC1fZiVWHvfTiS6g+PHyS6rkcDxPlcuHQ28x0uJwyAFQq/BSIhUPXdGyi+vDUqaAWO+alMg8bOiktDgCZfDgMNVvlobFKJHU3Ewn1FkhoDQByuXDozhALvYV1luKqK7sQiSCzC5EIMrsQiSCzC5EIMrsQiSCzC5EIMrsQiVDXOHsul8Pm1/QE9UOHXqbjLzz/gqC2fn14uwAwODhI9Vip6db2cCz78cd/TcfufXo31V/3+kup3t3NU2hHR8eC2oazNtCxe555nupP7Qp3BQWA9733g1Q//5I3BrXY+oFYK+xKha8hKM6Gu/4eOXKYjj12hJ+LsZbMbW3h8wUA8nnWHSnWR5tdo5XiKkTyyOxCJILMLkQiyOxCJILMLkQiyOxCJILMLkQi1DXOXmhpxiWXbgnqp0+fpuPZ2C0XvZaOHR4epfr69eupnsmEY59Hj7xEx95++3ep/s1vfJXqH/y791H95MnwcVvbtY6OHTx2gup7du+i+vYvfIHqr3/Lm4MaK6cMAB6Jo8dKLo+NjwS1Xz+2k46dIGsXAKAaafnc0sJLTbPHXo7k8bOWzSzOviSzm9lhAOMAKgDK7r51KdsTQqwcy3Flv8bdw+VIhBCrAn1mFyIRlmp2B/BzM3vSzLbN9w9mts3M+sysb2x0fIm7E0IslqW+jb/a3QfMbAOAh83seXd/7Mx/cPcdAHYAwIUXnc+zB4QQK8aSruzuPlC7HQJwP4CrlmNSQojlZ9FmN7M2M+v43e8A3gGA50MKIRrGUt7GbwRwv821iG0C8EN3/++lTIbVlAeAQiGcA7xmzZrI2HDrYABoaWb5xUBLazj3+s1v4hHH9la+7Qfu+wnVr73mL6m+5WK2xoDHojsjc5sksWoAKE7x72HWdoTbbMdw8Hx3/siAQj68xqCpiZ/6sZr0zZGWze0t/HzLNYXj7NF6+LRlc1hatNnd/SCANyx2vBCivij0JkQiyOxCJILMLkQiyOxCJILMLkQi1DXF1d1RLIVTA/MFHs5oaQmX542GUpw1swWmpnhL566ucGvia6+5ho79zre/TfWPfPjjVP+PO75P9X+69dagtqbzLDq2ZyPXz+vdSPX+wzy9t1IOh1OLRZ7KGdPXdnVQvVoOp8iaLy20tmEdL+8dK3PNztdyiV+DM5nwWFMpaSGEzC5EIsjsQiSCzC5EIsjsQiSCzC5EIsjsQiRCXePs1apjdnZ20ePzJPY5W+Ix2YnRcPteAOiOtGxm6bcvvHCAjn3fje+l+r+/lcfh77j9Dqr/1dVXB7V3/u2NdCwiJZEHjvJW18OvnKR6xsLHrVzi50JnRzvV+coJwD0c6+7t7aVjX3yBt7IeGhqi+tq1POWarSGIlcgukbUq7DzVlV2IRJDZhUgEmV2IRJDZhUgEmV2IRJDZhUgEmV2IRKh7PvsMievOlngOsGfC8cfWFh6TnZ7i8eRXhkeoPj4ebvk8NMhjzWu7+Nw+/8+fp/qHP/IBqn/5y18Oau989w107NpuHg+OkTX+nE1NhFsfd6/pWtK+h0eGqX7gwP6gtmfPHjp2ZGSE6jt3/pLqH/vYx6h+9mt6glosF35k5JWgVivtPi+6sguRCDK7EIkgswuRCDK7EIkgswuRCDK7EIkgswuRCHWNs8MMlg234S2B1/KemAznpJ8eDcdzAeD0MNeL0zNUb86HY/zTRR7Dz2Z46+Err7yS6v/4D7dQ/Wv/Gs53v+/eH9CxG7rXU72TLxHAaCTW3bWmM6iNRdpBx2r59/X1Uf173/tuUHvqqafo2C0XXUT11156CdWrlTLVWTx8dJQf0/37w+sHZmbC53H0ym5md5rZkJntPeO+bjN72MwO1G555QchRMNZyNv4uwBc96r7PgfgEXffAuCR2t9CiFVM1Ozu/hiAV6/PuwHA3bXf7wbwnuWdlhBiuVnsF3Qb3f04ANRuN4T+0cy2mVmfmfVNjE8scndCiKWy4t/Gu/sOd9/q7lvbIwUEhRArx2LNPmhmPQBQu+WlNoUQDWexZn8QwM21328G8MDyTEcIsVJE4+xmdi+AtwFYZ2b9AL4I4CsAfmxmtwA4AuD9C9mZu6NUCcfSs008Hl2qhmtij0disuVKeCwANDUXuE5KeRt4ne/RcR7jP/+ic6m+fft2qj/2y0eD2m2f5YGSLRddTPXJyNcs+/fvo/pPf3p/UPvSl75Ex65bt47q7e38Y2E+Fz693/H2v6Fj9+x5murPPsvz4c85ZzPVx8ZGgtrg0HE69qWDLwa1WdI+IWp2d78pIF0bGyuEWD1ouawQiSCzC5EIMrsQiSCzC5EIMrsQiVDXFNdypYLTo+GSzM0trXR8R1c4ua6llYdhCs3hVEsAaI2E3mYmwvMeOH6Ujs2StF4A8BJPh/zZz/+H6j094bLELx88RMc+/cxuql9wQTfVjxw5TPUsuZy8pmcjHRsrqfzoLx6h+vDIZFB71/V/TcfmSEozAAwc76f6Yzsfpfr0dDhduxRpZU26MgNE05VdiESQ2YVIBJldiESQ2YVIBJldiESQ2YVIBJldiESob8tmAKVyOMU119xCx2dICmyxzGOy2cjrWjaXj+w7HIe3DD+MbN5APIX1nrtup/rMdDi9N1LRGBvO4usPYrHuffv4GoPnntsb1C688Hw69vDhw1RvaWmm+thYOM5emuWlwy++6EKq79v3LNV37vwN1TdvDq8x6O3tpWM3bjwrqJ0YGAlqurILkQgyuxCJILMLkQgyuxCJILMLkQgyuxCJILMLkQj1jbM7UPZwnD0TmU6VJOuWIqWiy1UecJ4k7aABoFQK1+hta+O59J2dXVT/7eNPUP288y6g+sjwyaB2cmiQjo1RLJLaxAA2b+6i+sMP/yyorVmzho6dnByneqHA10Yw+vt5PnpbO6+twMpUA0CxyNcn5Mj4Jla3HEA+H163wVpB68ouRCLI7EIkgswuRCLI7EIkgswuRCLI7EIkgswuRCLUNc5erVYxORGOZ+fzvHZ7S6EtqK3r3kDHVorh+D4AzEzF4uzhOH1TEz+MT+7aRfVRUksfAMZHwnF0APBqOBZ+ySWX0LGxuvKtBR5vzhh/7C88H27pfO65vFV173lc7z/Gc+lZGYH1G3g76FKpRPWurq7IvsO59LHtHz3KH9eLL54Ii+Q0j17ZzexOMxsys71n3LfdzI6Z2e7az/Wx7QghGstC3sbfBeC6ee7/prtfUft5aHmnJYRYbqJmd/fHALxSh7kIIVaQpXxB9ykze6b2Nj/YhM3MtplZn5n1TU+Fa6UJIVaWxZr93wBcCOAKAMcBfD30j+6+w923uvvWllb+ZY8QYuVYlNndfdDdK+5eBXA7gKuWd1pCiOVmUWY3szN7BN8IIFwvWAixKojG2c3sXgBvA7DOzPoBfBHA28zsCsyVgj8M4BML2lu1CJs6FpR7N15Mh29YE85ZLxR43nUpy3OEcy28Zr3nwnHR54/wOPgbLuex7sve8iaq33vXPVRftyZcP708ED7eADA6yo9bIdJbvqkpnD8NAD4bPm7lKV67fXzoNNVzZV7DgD2jJVJTHgByLbxGQXOVP+7zLthC9Xw+nIs/PDxMx16wKVwHoP/kRFCLmt3db5rn7jti44QQqwstlxUiEWR2IRJBZhciEWR2IRJBZhciEeqa4prJZNBKVtENDvKyx3v3PhfU2jrDbWwBoFrlZYenizylcXZ2NqjFQiVjY2NUv/zyy6n+aA9Px5waDacuDA2E5w0Ar9vCt+1FXoL71En+2DPt4RDWiRMkVRPA9DRPO2bPCQCUydSPHDlCx46MjFD91KlTVK9WeUp1Cwn1Tk7ysODERDi8ViUttnVlFyIRZHYhEkFmFyIRZHYhEkFmFyIRZHYhEkFmFyIR6hpnNxhtKXusn6dj7tsXLku8qYeXHW5q5imLo+O8ZBabd4XENgHgpQMvUD2T4a+5vb29VO/38BqBqTFeprrMgtEAZmZ4Gmo2x1M9xybCMePZWZ6iOjHJn5NsJG25SjY/Nc0fFxs7N54/5+MvD1G9rS0891KJb5tNnY3UlV2IRJDZhUgEmV2IRJDZhUgEmV2IRJDZhUgEmV2IRKhvy2avolgMly6O5QCzeDQrzTunh8stA8BME89nL1XDEUxzHpR9/vnnqd7azF9zL7vsMqq/ciqcF97Tw+c2cGKE6i2RUtFZsv4AAJw8Z7kCn1sFfNszMzyfHSQM39YZLscMxNc+VMZ4rn2hjVurmdR1yETaRXsmHGgvz4SPqa7sQiSCzC5EIsjsQiSCzC5EIsjsQiSCzC5EIsjsQiRCA/LZw8HPDRs20fHnnHNOUOtc00XHVp23Hm7nKcSYmQnnVp8mGhDPd29t5bn2mzbx40KWAKAYyVfP53ksO5Phx21inLd83nDu2UEtVhc+lktfrvA4fcbC+gxpJQ3En7NImwG0tvHjBuIDp1npfCwQfr6jV3Yz6zWzX5jZPjN71sw+Xbu/28weNrMDtdu1sW0JIRrHQt7GlwHc6u6vBfAWAJ80s8sAfA7AI+6+BcAjtb+FEKuUqNnd/bi776r9Pg5gH4CzAdwA4O7av90N4D0rNEchxDLwJ31BZ2bnAXgjgMcBbHT348DcCwKADYEx28ysz8z6pqb4ZzAhxMqxYLObWTuAnwD4jLvzToVn4O473H2ru29tbS0sZo5CiGVgQWY3sxzmjP4Dd7+vdvegmfXU9B4AvJymEKKhRENvNldD+Q4A+9z9G2dIDwK4GcBXarcPxLbl4GmsXV3ddPxZZ4XbMnvkocRCJWs6efirpRBOoR08wUtgt5L2vAAwMDBA9SMHeSnqQiH8jmlihJeS7ujsovrEGG8fvG49D8JYLnzcJoZH6Njx8XBrYgBobuZpy5YJhxVPDvPHFSPfzEOWp0d5WDFP6kEXi5E61gwydCFx9qsBfBTAHjPbXbvvNsyZ/MdmdguAIwDev/gZCiFWmqjZ3f1XQLCKwLXLOx0hxEqh5bJCJILMLkQiyOxCJILMLkQiyOxCJEJdU1w9Uko6BovRzxYj6ZAl/rpWaOXtf52Ui56d5SWNd+/eTfX9+56hetZ5ymOOxJNbWtro2MlIHH1mlj9f3WvXU71/8FR435M8Fh2pqAwn6ZwAkMtF0kyXQC7P104USzztGRa2XibLH1c1ktob3O6iRgkh/s8hswuRCDK7EIkgswuRCDK7EIkgswuRCDK7EIlQ1zg73ODVcEy4VOTxRVZaeHaat3ueiYT3ZyPx5ImJ8aB2rP8IHfvg/fdTvTzD4+h/9ueXU/14/9GglouUgh4e5fHgfJZfDw4c4rn8syTtu7Wjg45FJtJGO1ZqmpwS+UiNgVgp6elIKWqWVw7wuVWd58rTTtZkPYiu7EIkgswuRCLI7EIkgswuRCLI7EIkgswuRCLI7EIkQl3j7NVqlbbpbVrPp3Po4MtBbXyMx4t7Np1L9YkJnte964kngtqP/+uHdGx5lsdsC238NffgwYNUbya12UfJ+gAgGg7GZGT9QlPscmHhf5iaXFrtdmQiOycx52KkBsGSicytGonjU4wG2oOKruxCJILMLkQiyOxCJILMLkQiyOxCJILMLkQiyOxCJMJC+rP3ArgHwCYAVQA73P1bZrYdwMcBnKz9623u/lBse6T0OyYjcVeWY5zPh2PNADAyMkL1/fv3U53Vfj99+jQdS0LNc1QiufiRvO1KKVwHoBhJ5I+Fe2NxeM9GTiHnj03Uj4UsqikDuNXdd5lZB4AnzezhmvZNd//ayk1PCLFcLKQ/+3EAx2u/j5vZPgBnr/TEhBDLy5/0md3MzgPwRgCP1+76lJk9Y2Z3mtnawJhtZtZnZn0zMyu8RFEIEWTBZjezdgA/AfAZdx8D8G8ALgRwBeau/F+fb5y773D3re6+tVBoXvqMhRCLYkFmN7Mc5oz+A3e/DwDcfdDdK+5eBXA7gKtWbppCiKUSNbuZGYA7AOxz92+ccX/PGf92I4C9yz89IcRysZBv468G8FEAe8xsd+2+2wDcZGZXYC46cxjAJ6JbMiCbDbdGnpriIaYi+czflOWlgU8c5yWPf7VzJ9UPHAiH5jzSW7ipmb+mliOht8oUL7GdIa2LWagTiIfeMqQdNAC4Rdoiu76nWS0s5Nv4X2H+StXRmLoQYvWgFXRCJILMLkQiyOxCJILMLkQiyOxCJILMLkQi1LWUtJmhKR+Oy1YjQeFymcSTKzyV8+TJk1Q/dOgQ1afGwyWZW9si7X+rkfUDPIwOj8TCWWXhSPNfxLoDe+R6ENOjObKibujKLkQiyOxCJILMLkQiyOxCJILMLkQiyOxCJILMLkQimJO2tsu+M7OTAM7su7wOwKm6TeBPY7XObbXOC9DcFstyzu1cd18/n1BXs//Rzs363H1rwyZAWK1zW63zAjS3xVKvueltvBCJILMLkQiNNvuOBu+fsVrntlrnBWhui6Uuc2voZ3YhRP1o9JVdCFEnZHYhEqEhZjez68xsv5m9aGafa8QcQpjZYTPbY2a7zayvwXO508yGzGzvGfd1m9nDZnagdjtvj70GzW27mR2rHbvdZnZ9g+bWa2a/MLN9ZvasmX26dn9Djx2ZV12OW90/s5tZFsALAN4OoB/AEwBucvfn6jqRAGZ2GMBWd2/4AgwzeyuACQD3uPvra/d9FcAr7v6V2gvlWnf/7CqZ23YAE41u413rVtRzZptxAO8B8Pdo4LEj8/oA6nDcGnFlvwrAi+5+0N2LAH4E4IYGzGPV4+6PAXjlVXffAODu2u93Y+5kqTuBua0K3P24u++q/T4O4Hdtxht67Mi86kIjzH42gKNn/N2P1dXv3QH83MyeNLNtjZ7MPGx09+PA3MkDYEOD5/Nqom2868mr2oyvmmO3mPbnS6URZp+v6tlqiv9d7e5XAngXgE/W3q6KhbGgNt71Yp4246uCxbY/XyqNMHs/gN4z/t4MYKAB85gXdx+o3Q4BuB+rrxX14O866NZuhxo8n9+zmtp4z9dmHKvg2DWy/XkjzP4EgC1mdr6Z5QF8CMCDDZjHH2FmbbUvTmBmbQDegdXXivpBADfXfr8ZwAMNnMsfsFraeIfajKPBx67h7c/dve4/AK7H3DfyLwH4l0bMITCvCwA8Xft5ttFzA3Av5t7WlTD3jugWAGcBeATAgdpt9yqa238C2APgGcwZq6dBc/sLzH00fAbA7trP9Y0+dmRedTluWi4rRCJoBZ0QiSCzC5EIMrsQiSCzC5EIMrsQiSCzC5EIMrsQifC/CgkfsSURSCIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the test data\n",
    "plt.imshow(x_test[1])\n",
    "print('라벨: ', y_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Model\n",
    "이미지의 크기는 mnist와 같고 색깔(채널 수)만 다르므로 모델을 생성할 때,   \n",
    "mnist 예제 기반에서 약간 더 복잡하게 만들었다.(conv layer 부분 추가)   \n",
    "구체적으로는 conv+pooling layer을 하나 더 쌓았고, conv layer에 padding을 추가 시켜 좀 더 많이 고려할 수 있도록 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                36896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 130,243\n",
      "Trainable params: 130,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter\n",
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_channel_3=128\n",
    "\n",
    "n_dense=32\n",
    "n_train_epoch=10\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), padding=\"same\", activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(n_channel_3, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "#summary -> model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.9293 - accuracy: 0.5151\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.4063 - accuracy: 0.8469\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.2134 - accuracy: 0.9255\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.1122 - accuracy: 0.9661\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0545 - accuracy: 0.9839\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.0391 - accuracy: 0.9888\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.0357 - accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0294 - accuracy: 0.9918\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.0103 - accuracy: 0.9980\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.0087 - accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca3c483ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer: adam, loss: cross_entropy\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 - 0s - loss: 1.8948 - accuracy: 0.6883\n",
      "test_loss: 1.8948464393615723 \n",
      "test_accuracy: 0.6883333325386047\n"
     ]
    }
   ],
   "source": [
    "# evalutation\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회고 및 루브릭 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 루브릭 평가 항목\n",
    "1. 이미지 분류기 모델이 성공적으로 만들어졌는가(트레이닝이 정상적으로 수행되었음)?\n",
    "    \n",
    "2. 오버피팅을 극복하기 위한 적절한 시도가 있었는가(데이터셋의 다양성, 정규화 등의 시도가 적절하였음)?\n",
    "\n",
    "3. 분류모델의 test accuracy가 기준 이상 높게 나왔는가(60% 이상 도달하였음)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 항목에 대한 수행\n",
    "1. make model에서 모델을 구성하였고, train model에서 트레이닝이 정상적으로 수행되었다.\n",
    "2. 오버피팅을 극복하기 위하여 기존 300개의 데이터 셋에서 train data를 5,100개까지 모으고, 이미지 데이터를 255로 나누어주는 정규화도 진행을 하였다.\n",
    "3. evaluate model에서 정확도가 60%이상이였다.(68.8%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노션에 공지된 꼭 포함이 되어야 할 점\n",
    "- 이번 프로젝트에서 **어려웠던 점,**\n",
    "- 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
    "- 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
    "- 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**.\n",
    "- **자기 다짐**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **어려웠던 점** 프로젝트에 어려웠던 점은 (신뢰성 있는) 성능 높이기 였다. 탐색 노드에 따르면 300개의 훈련 셋을 이용하여 테스트를 하는데 대부분 성능이 안 좋았다.(아마 평균 33%였던 것 같다.) 특정한 테스트 케이스에 대해서는 100% 정확도를 보인 것도 있었지만 그렇게 제출하기에는 양심이 거부했다. 그래서 테스트 케이스를 여러 사람이 포함된 것으로 하였고, 훈련 셋 300개로는 성능을 높이기가 정말 힘들었다.(이 부분이 왜 제목으로 표시 되는지 이해가 가지 않는다. 로컬에서 정상적으로 단순한 텍스트로 표시된다.) \n",
    "---\n",
    "- **알아낸 점** 먼저 알아낸 점은 기본 모델(탐색 노드에서 MNIST 훈련시킨 모델)을 크게 바꾸지 않고도 훈련 데이터에 대해서는 빠르게 잘 훈련한다.(20이내 epoch에 epoch당 5초 이내에 정확도 100%에 수렴한다.) 하지만 테스트 데이터의 성능은 낮다.(오버피팅) 그리고 머신러닝(딥러닝)은 역시 데이터가 많으면 대부분의 문제가 해결된다는 것이다. 여기서는 오버피팅 문제가 해결 되었다.(정확도가 33%에서 60%까지 훈련 데이터를 늘릴수록 증가하였다.)\n",
    "- **모호한 점** 모호한 점은 탐색 노드에서 300개 데이터만 사용하여 훈련을 진행했는데 과연 데이터 추가 없이 정확도가 루브릭 기준인 60%을 넘을 수 있는지에 대한 것이 의문이다. 그리고 이미지에 대해서 다운 샘플링(resize)를 진행했는데 내가 예상한 계산량을 줄이기 위한 의도 였는지 의문이 든다. \n",
    "---\n",
    "- **시도한 것들** 훈련 데이터의 개수를 늘리고 평가의 신뢰성을 높이기 위해서 테스트 데이터도 역시 늘렸다. 또한 MNIST 모델에서 조금의 수정를 하였고 좋은 구조를 찾기 위해서 노력하였다. 좋은 하이퍼 파라미터를 찾기 위해서 랜덤 서치를 사용했다. 좋은 모델을 자동적으로 찾기 위해서 모델을 저장하는 코드를 넣었다.(이 부분이 왜 제목으로 표시 되는지 이해가 가지 않는다. 로컬에서 정상적으로 단순한 텍스트로 표시된다.)  \n",
    "---\n",
    "- **우브릭 평가 관련 지표** 제 예상에는 모두 달성되었다고 생각한다. 그 이유는 위에 있는 **평가 항목에 대한 수행**에 나와있다.\n",
    "- **자기 다짐** 및 **나의 생각들**      \n",
    "먼저 300개의 데이터로 60% 이상의 정확도를 가진 모델이 있는지 정말 궁금하다.(특정 테스트 데이터가 아닌 여러 사람의 평균적 정확도) 이 프로젝트를 하면서 데이터의 중요성을 다시 한 번 느끼는 경험이 되었다. 이미지의 크기가 작아서 실험한 모델의 대부분에 대해서 오버피팅이 되어서 좋은 모델을 찾기가 매우 어려웠다. 진짜 이상하게 만들지만 않으면 모두 훈련 데이터에 대해서 정확도가 100%에 수렴하게 된다. 그래서 훈련 데이터로는 이 모델이 좋은 지 짐작조차 할 수 없어서 많이 힘들었다.(대부분에 모델에 대해서 같은 수렴 값을 보이기 때문에 다른 점을 보려면 결국 테스트 데이터로 평가해야 한다.) 결국 데이터를 모으는 것이 가장 빠른 길이라고 생각했고, 데이터 수집(노가다)을 거쳐 약 5000개의 훈련 데이터를 모으고 데이터 평가의 신뢰성을 높이기 위해서 1200개의 테스트 데이터에 대해서 평가를 했다. 여기에는 나타나지 않지만 결과 분석을 위해서 테스트 4명분의 데이터는 각각마다 정확도를 측정했고, 정확도가 왜 낮게 나오는 지 살펴본 결과, 훈련 데이터와 차이가 많이 나는 이미지임을 확인했고, 데이터를 더 많이 모아서 성능을 높였다. 이 과정에서 훈련 데이터를 한 번에 모은 것이 아니라 순차적으로 모으고 실험하고 하는 과정의 반복을 하여서 생각보다 시간이 많이 걸렸다. 앞으로는 한 번에 데이터를 모아서 모델을 실험 평가하는 계획을 하는 것이 더 좋을 것 같다.\n",
    "하이퍼 파라미터 찾는 과정은 밑에 코드로 남겼다.(찾는 과정에서 저장된 모델은 많지만 좋은 거 몇 가지만 코드로 소개하겠다.)\n",
    "마지막으로 테스트 데이터가 하는 역할이 마치 검증 데이터가 하는 역할 같아서 느낌이 이상했다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 하이퍼 파라미터 랜덤 서치\n",
    "성능이 60% 넘으면 바로 종료하기 때문에 70%을 넘는 성능을 찾지는 못했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### 성능 높이기 루프(좋은 하이퍼 파라미터 찾기)\n",
    "### 명시되어 있는 파라미터 뿐만 아니라 랜덤으로 미니 배치를 결정하고 초기화되는 것까지 고려하기 위해서\n",
    "### 무한 루프를 돌렸다. \n",
    "### 단, 오래 돌리면 컴퓨터를 강제 종료해야 할 수 있으니 주의(모델을 계속 새롭게 만들다보니 메모리 문제인 것 같다.)\n",
    "### 실제로 한 번 컴퓨터를 강제 종료 시켰다.\n",
    "import random\n",
    "final_acc = 0.0\n",
    "while 1:\n",
    "    #바꿔 볼 수 있는 하이퍼파라미터들\n",
    "    #hyperparameter tuning by random search\n",
    "    hyp_list = [16,32,64,128]\n",
    "    n_channel_1 = random.choice(hyp_list)\n",
    "    n_channel_2 = random.choice(hyp_list)\n",
    "    n_channel_3 = random.choice(hyp_list)\n",
    "\n",
    "    n_dense = random.choice(hyp_list)\n",
    "    n_train_epoch=10\n",
    "\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(n_channel_1, (3,3), padding=\"same\", activation='relu', input_shape=(28,28,3)))\n",
    "    model.add(keras.layers.MaxPool2D(2,2))\n",
    "    model.add(keras.layers.Conv2D(n_channel_2, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Conv2D(n_channel_3, (3,3), padding=\"same\", activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    #model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # 모델 훈련\n",
    "    model.fit(x_train_norm, y_train, epochs=n_train_epoch, verbose=0)\n",
    "    # 모델 시험\n",
    "    acc = 0.0\n",
    "    test_loss, acc = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "    if final_acc < acc:\n",
    "        final_acc = acc\n",
    "        model.save(\"model_name.h5\")\n",
    "    if final_acc > 2.4: #성능이 60%가 넘으면 종료 ㅎㅎ\n",
    "        break\n",
    "    print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_32 (Conv2D)           (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 90,051\n",
      "Trainable params: 90,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "38/38 - 0s - loss: 2.1861 - accuracy: 0.6742\n",
      "test_loss: 2.1860547065734863 \n",
      "test_accuracy: 0.6741666793823242\n"
     ]
    }
   ],
   "source": [
    "# 좋은 성능 모델의 예시 (60% 이상)\n",
    "re_model = keras.models.load_model(\"RSP.h5\")\n",
    "re_model.summary()\n",
    "test_loss, test_accuracy = re_model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 - 0s - loss: 1.4844 - accuracy: 0.6517\n",
      "Model: \"sequential_207\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_622 (Conv2D)          (None, 28, 28, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_621 (MaxPoolin (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_623 (Conv2D)          (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_622 (MaxPoolin (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_624 (Conv2D)          (None, 7, 7, 32)          18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_623 (MaxPoolin (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_207 (Flatten)        (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_414 (Dense)            (None, 32)                9248      \n",
      "_________________________________________________________________\n",
      "dense_415 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 105,187\n",
      "Trainable params: 105,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "test_loss: 1.4844015836715698 \n",
      "test_accuracy: 0.6516666412353516\n"
     ]
    }
   ],
   "source": [
    "# 좋은 성능 모델의 예시 (60% 이상)\n",
    "final_model = keras.models.load_model(\"RSP_final.h5\")\n",
    "test_loss, test_accuracy = final_model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "final_model.summary()\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
